
# This file is used to create a FastAPI app that serves as a chatbot interface.
import gradio as gr
from fastapi import FastAPI
from langchain_openai import ChatOpenAI
from langchain.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_neo4j import Neo4jGraph, GraphCypherQAChain
from langchain_core.output_parsers import StrOutputParser
from gradio import ChatMessage
from langchain.prompts import PromptTemplate

import asyncio
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI
app = FastAPI()

# Initialize Neo4j with timeout
try:
    graph = Neo4jGraph(
        url="bolt://localhost:7687",
        username="neo4j",
        password="cznb6666",
        database="neo4j",
        timeout=100
    )
    logger.info("Successfully connected to Neo4j")
except Exception as e:
    logger.error(f"Failed to connect to Neo4j: {e}")
    graph = None

# Initialize LangChain components
llm = ChatOpenAI(
    temperature=1.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡º
    model="deepseek-chat",
    openai_api_key="sk-44858716b14c48ebba036313015e4584",
    openai_api_base="https://api.deepseek.com"
)

# ä¿®å¤æç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}")
])

chain = prompt | llm | StrOutputParser()

# ä¿®å¤GraphCypherQAChainåˆå§‹åŒ–
if graph:
    try:
        graph.refresh_schema()
        logger.info("Neo4j schema refreshed successfully")
        # åœ¨åˆå§‹åŒ–graph_qaä¹‹å‰æ·»åŠ è‡ªå®šä¹‰prompt
        CYPHER_GENERATION_PROMPT = PromptTemplate(
            input_variables=["schema", "question"],
            template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Neo4j CypheræŸ¥è¯¢ç”Ÿæˆå™¨ã€‚è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

          é‡è¦è§„åˆ™ï¼š
           1. ğŸ”´ ç»å¯¹ç¦æ­¢åœ¨å±æ€§åä¸­ä½¿ç”¨ç‚¹å·(.) - è¿™æ˜¯è¯­æ³•é”™è¯¯ï¼
           2. å±æ€§ååªèƒ½ä½¿ç”¨å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼š[a-zA-Z0-9_]
           3. ç¡®ä¿æ‰€æœ‰è¯­æ³•ç¬¦åˆNeo4jæ ‡å‡†
           4. åªè¿”å›CypheræŸ¥è¯¢è¯­å¥ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Š
           5. ä½ è¿˜å¯ä»¥ä½¿ç”¨Neo4jçš„Spatialæ’ä»¶å’ŒApocæ’ä»¶æä¾›çš„procedureså’Œfunctions

          æ•°æ®åº“Schemaï¼š
           {schema}

          ç”¨æˆ·é—®é¢˜ï¼š{question}

          è¯·ç”Ÿæˆ100%è¯­æ³•æ­£ç¡®çš„CypheræŸ¥è¯¢ï¼š"""
        )

        CYPHER_QA_PROMPT = PromptTemplate(
            input_variables=["context", "question"],
            template="""åŸºäºä»¥ä¸‹æ•°æ®åº“æŸ¥è¯¢ç»“æœï¼Œç”¨ä¸­æ–‡æ¸…æ™°å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

          æŸ¥è¯¢ç»“æœï¼š
          {context}

          ç”¨æˆ·é—®é¢˜ï¼š{question}

           è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼š"""
        )
        graph_qa = GraphCypherQAChain.from_llm(
            llm=llm,
            graph=graph,
            validate_cypher=True,
            verbose=True,
            return_direct=False,  # è®©LLMå¤„ç†ç»“æœè€Œä¸æ˜¯ç›´æ¥è¿”å›
            allow_dangerous_requests=True,
            cypher_prompt=CYPHER_GENERATION_PROMPT,  # æ·»åŠ è‡ªå®šä¹‰cypher prompt
            qa_prompt=CYPHER_QA_PROMPT  # æ·»åŠ è‡ªå®šä¹‰QA prompt
        )
        logger.info("GraphCypherQAChain initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize GraphCypherQAChain: {e}")
        graph_qa = None
else:
    graph_qa = None
    logger.info(
        "No Neo4j connection, skipping GraphCypherQAChain initialization")

# å­˜å‚¨ä¼šè¯å†å²
store = {}


def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# ä¿®å¤çš„èŠå¤©å‡½æ•°


async def chat_with_timeout(message: str, history):
    try:
        config = {"configurable": {"session_id": "default_session"}}

        # å¦‚æœæœ‰å›¾æ•°æ®åº“ä¸”é—®é¢˜å¯èƒ½æ¶‰åŠç©ºé—´æŸ¥è¯¢
        if graph_qa:
            try:
                # # ä½¿ç”¨æ­£ç¡®çš„CypheræŸ¥è¯¢æ ¼å¼
                # corrected_message = message.replace("'", "\\'")  # è½¬ä¹‰å•å¼•å·
                graph.refresh_schema()
                input_key = graph_qa.input_keys[0] if graph_qa.input_keys else 'query'

                # å¼‚æ­¥æ‰§è¡Œå›¾æŸ¥è¯¢
                neo4j_response = await asyncio.wait_for(
                    asyncio.to_thread(
                        graph_qa.invoke,
                        {input_key: message}
                    ),
                    timeout=100
                )
                print(f"Neo4j response:")
                response = f"æ ¹æ®æ•°æ®åº“ï¼š{neo4j_response}"

            except asyncio.TimeoutError:
                print("Neo4j query timeout")
                response = "æ•°æ®åº“æŸ¥è¯¢è¶…æ—¶ï¼Œè¯·å°è¯•æ›´ç®€å•çš„é—®é¢˜"
            except Exception as e:
                print(f"Neo4j query error: {e}")
                logger.error(f"Neo4j query error: {e}")
                response = "æ•°æ®åº“æŸ¥è¯¢å‡ºé”™ï¼Œä½¿ç”¨é€šç”¨å›ç­”"

                # å›é€€åˆ°æ™®é€šLLM
                llm_response = await chain_with_history.ainvoke(
                    {"input": message},
                    config=config
                )
                response = llm_response
        else:
            # ç›´æ¥ä½¿ç”¨LLM
            print("common LLM response")
            llm_response = await chain_with_history.ainvoke(
                {"input": message},
                config=config
            )
            response = llm_response

        history.append(ChatMessage(role="user", content=message))
        history.append(ChatMessage(role="assistant", content=response))

        return history

    except Exception as e:
        print(f"Error in chat function: {e}")
        logger.error(f"Error in chat function: {e}")
        error_msg = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜"
        history.append(ChatMessage(role="user", content=message))
        history.append(ChatMessage(role="assistant", content=error_msg))
        return history

# åˆ›å»ºGradioç•Œé¢
with gr.Blocks() as demo:
    gr.Markdown("# ChatBot")
    chatbot = gr.Chatbot(type="messages", height=500)
    msg = gr.Textbox(label="è¾“å…¥æ‚¨çš„é—®é¢˜", placeholder="è¯·è¾“å…¥...")
    clear = gr.ClearButton([msg, chatbot])

    def respond(message, chat_history):
        # åŒæ­¥è°ƒç”¨å¼‚æ­¥å‡½æ•°
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(
                chat_with_timeout(message, chat_history))
            return result
        finally:
            loop.close()

    msg.submit(respond, [msg, chatbot], [chatbot])
    msg.submit(lambda: "", None, [msg])  # æ¸…ç©ºè¾“å…¥æ¡†

# Mount Gradio app to FastAPI
app = gr.mount_gradio_app(app, demo, path="/")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


# # This file is used to create a FastAPI app that serves as a chatbot interface.
# import gradio as gr
# from fastapi import FastAPI, HTTPException, Request
# from pydantic import BaseModel
# from langchain_openai import ChatOpenAI
# from langchain.prompts import (
#     ChatPromptTemplate,
#     MessagesPlaceholder,
#     SystemMessagePromptTemplate,
#     HumanMessagePromptTemplate,
# )
# from langchain_core.runnables.history import RunnableWithMessageHistory
# from langchain_community.chat_message_histories import ChatMessageHistory
# # from langchain.chains import LLMChain
# from langchain.memory import ConversationBufferMemory
# from langchain_neo4j import Neo4jGraph,GraphCypherQAChain
# # from langchain.chains import GraphCypherQAChain
# from langchain_core.output_parsers import StrOutputParser
# from gradio import ChatMessage

# import asyncio
# from typing import List
# import json

# # Initialize FastAPI
# app = FastAPI()

# # Initialize Neo4j with timeout
# try:
#     graph = Neo4jGraph(
#         url="bolt://localhost:7687",
#         username="neo4j",
#         password="cznb6666",
#         database="neo4j",
#         timeout=100  # 60 seconds timeout
#     )
# except Exception as e:
#     print(f"Failed to connect to Neo4j: {e}")
#     graph = None

# # Fallback in-memory storage
# # job_seekers = []
# # job_positions = []

# # Initialize LangChain components
# llm = ChatOpenAI(
#     temperature=0.95,
#     model="deepseek-chat",
#     openai_api_key="sk-44858716b14c48ebba036313015e4584",
#     openai_api_base="https://api.deepseek.com"
# )

# prompt = ChatPromptTemplate(
#     messages=[
#         SystemMessagePromptTemplate.from_template(
#             "You are a helpful AI assistant working as a guide for WuHan University. You can answer questions about wuhan university."
#         ),
#         MessagesPlaceholder(variable_name="chat_history"),
#         HumanMessagePromptTemplate.from_template("{question}")
#     ]
# )

# # memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
# # conversation = LLMChain(
# #     llm=llm,
# #     prompt=prompt,
# #     verbose=True,
# #     memory=memory
# # )


# chain = prompt | llm  | StrOutputParser()

# # # Initialize GraphCypherQAChain if Neo4j is available
# if graph:
#     graph_qa = GraphCypherQAChain.from_llm(
#         llm,
#         graph=graph,
#         verbose=True,
#         allow_dangerous_requests=True  # æ˜ç¡®ç¡®è®¤äº†è§£é£é™©

#     )

# # 3. åˆ›å»ºä¸€ä¸ªå­˜å‚¨æ¥ç®¡ç†ä¸åŒä¼šè¯çš„å†å²è®°å½•
# # è¿™é‡Œä½¿ç”¨ç®€å•çš„å­—å…¸åœ¨å†…å­˜ä¸­å­˜å‚¨ï¼Œå¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®ä½¿ç”¨æ•°æ®åº“
# store = {}

# def get_session_history(session_id: str) -> ChatMessageHistory:
#     if session_id not in store:
#         store[session_id] = ChatMessageHistory()
#     return store[session_id]

# # 4. ä½¿ç”¨ RunnableWithMessageHistory åŒ…è£…æ ¸å¿ƒé“¾
# chain_with_history = RunnableWithMessageHistory(
#     chain,
#     get_session_history, # æä¾›è·å–å†å²è®°å½•çš„å‡½æ•°
#     input_messages_key="input", # è¾“å…¥ä¸­ç”¨æˆ·é—®é¢˜çš„é”®
#     history_messages_key="chat_history", # æç¤ºæ¨¡æ¿ä¸­å†å²æ¶ˆæ¯çš„å˜é‡å
# )

# # 5. è°ƒç”¨æ—¶ï¼Œéœ€è¦ä¼ å…¥ config æ¥æŒ‡å®šä¼šè¯ ID
# # è¿™æ ·ï¼ŒåŒä¸€ä¸ª session_id çš„å¯¹è¯ä¼šè‡ªåŠ¨å…±äº«å†å²
# config = {"configurable": {"session_id": "user_123"}} # å¯ä»¥ä¸ºæ¯ä¸ªç”¨æˆ·æˆ–ä¼šè¯è®¾ç½®å”¯ä¸€çš„ ID

# # Define chat function with timeout
# async def chat_with_timeout(message, history):
#     try:
#         if graph:
#             neo4j_response = await asyncio.wait_for(
#                 asyncio.to_thread(graph_qa.run, message),
#                 timeout=30.0  # 10 seconds timeout
#             )
#             history.append(ChatMessage(role="assistant", content=f"Based on our database: {neo4j_response}"))
#         else:
#             response = chain_with_history.invoke({"question": message})
#             history.append(ChatMessage(role="assistant", content=response.get('text', 'No response')))
#         return history
#     except asyncio.TimeoutError:
#         history.append(ChatMessage(role="assistant", content="I'm sorry, but the database query took too long. Please try a simpler question or try again later."))
#         return history
#     except Exception as e:
#         print(f"Error in chat function: {e}")
#         history.append(ChatMessage(role="assistant", content="An error occurred. Please try again later."))
#         return history


# # # Create Gradio interface
# # iface = gr.ChatInterface(chat_with_timeout,type='messages')
# # Create Gradio interface
# with gr.Blocks() as demo:
#     chatbot = gr.Chatbot(type="messages")
#     msg = gr.Textbox()
#     clear = gr.ClearButton([msg, chatbot])

#     msg.submit(chat_with_timeout, [msg, chatbot], [chatbot])

# #
# # # Mount Gradio app to FastAPI
# app = gr.mount_gradio_app(app, demo, path="/")

# # Run the app
# if __name__ == "__main__":
#     import uvicorn

#     uvicorn.run(app, host="0.0.0.0", port=8000)
