#!/usr/bin/env python3
"""
æµ‹è¯•LangGraphçš„interruptåŠŸèƒ½ï¼ˆæ–°ç‰ˆæœ¬ï¼‰
æµ‹è¯•ä½¿ç”¨å†…ç½®çš„interruptå’ŒCommandæœºåˆ¶
"""

import json
import logging
from typing import Dict, Any
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import interrupt, Command

from core.schemas import AgentState
from core.graph.nodes import build_node_context, build_node_mapping
from core.graph.edges import should_interrupt_after_intent

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def setup_test_graph():
    """è®¾ç½®æµ‹è¯•ç”¨çš„ç®€åŒ–LangGraph"""

    # åˆ›å»ºåŸºæœ¬çš„æµ‹è¯•æ¶æ„
    context = build_node_context(
        sql_generator=None,
        sql_executor=None,
        result_parser=None,
        answer_generator=None,
        schema_fetcher=None,
        llm=None,
        error_handler=None,
        cache_manager=None,
        structured_logger=None
    )

    node_handlers = build_node_mapping(context)

    # åˆ›å»ºç®€åŒ–çš„æµ‹è¯•å›¾
    workflow = StateGraph(AgentState)

    # æ·»åŠ æµ‹è¯•èŠ‚ç‚¹
    def analyze_intent_node(state: AgentState) -> Dict[str, Any]:
        print(f"ğŸ” åˆ†ææ„å›¾: '{state['query']}'")

        # ç®€åŒ–çš„æ„å›¾åˆ†æ
        is_clear = len(state['query']) > 5 and any(word in state['query'] for word in ['çš„', 'ç»Ÿè®¡', 'æŸ¥è¯¢', 'å¤šå°‘'])

        intent_info = {
            "intent_type": "query",
            "is_spatial": False,
            "confidence": 0.8 if is_clear else 0.3,
            "reasoning": f"æŸ¥è¯¢åŒ…å«è¶³å¤Ÿä¿¡æ¯: {is_clear}",
            "is_query_clear": is_clear,
            "keywords_matched": ["æµ‹è¯•"]
        }

        return {
            "query_intent": "query",
            "requires_spatial": False,
            "intent_info": intent_info,
            "thought_chain": [{
                "step": 1,
                "type": "intent_analysis",
                "action": "analyze_intent",
                "input": state['query'],
                "output": intent_info,
                "status": "completed"
            }]
        }

    def interrupt_check_node(state: AgentState) -> Dict[str, Any]:
        print(f"ğŸ”„ æ£€æŸ¥interrupt: '{state['query']}'")

        intent_info = state.get("intent_info", {})
        is_query_clear = intent_info.get("is_query_clear", True)

        if not is_query_clear:
            print("â“ æŸ¥è¯¢ä¸æ˜ç¡®ï¼Œè¯·æ±‚æ¾„æ¸…...")
            new_query = interrupt({
                "original_query": state['query'],
                "message": "æ‚¨çš„æŸ¥è¯¢ä¸å¤Ÿæ˜ç¡®"
            })
            print(f"âœ… è·å¾—æ–°æŸ¥è¯¢: '{new_query}'")

            return {
                "query": new_query,
                "interrupt_info": {
                    "interrupted": True,
                    "reason": "query_clarified",
                    "new_query": new_query
                },
                "thought_chain": [{
                    "step": 2,
                    "type": "interruption",
                    "action": "request_clarification",
                    "input": state['query'],
                    "output": {"new_query": new_query},
                    "status": "completed"
                }]
            }
        else:
            return {
                "interrupt_info": {"interrupted": False},
                "thought_chain": [{
                    "step": 2,
                    "type": "interruption",
                    "action": "query_clear",
                    "input": state['query'],
                    "output": {"query_clear": True},
                    "status": "completed"
                }]
            }

    def enhance_query_node(state: AgentState) -> Dict[str, Any]:
        print(f"â­ å¢å¼ºæŸ¥è¯¢: '{state['query']}'")
        return {
            "enhanced_query": state['query'] + " å¢å¼ºç‰ˆ",
            "message": f"å·²å¢å¼ºæŸ¥è¯¢: {state['query']}"
        }

    def generate_answer_node(state: AgentState) -> Dict[str, Any]:
        print(f"âœ¨ ç”Ÿæˆç­”æ¡ˆ: '{state['enhanced_query']}'")
        return {
            "answer": f"æ‚¨çš„æŸ¥è¯¢ '{state['query']}' çš„ç­”æ¡ˆæ˜¯...",
            "status": "completed",
            "final_data": []
        }

    # æ·»åŠ èŠ‚ç‚¹åˆ°å·¥ä½œæµ
    workflow.add_node("analyze_intent", analyze_intent_node)
    workflow.add_node("interrupt_check", interrupt_check_node)
    workflow.add_node("enhance_query", enhance_query_node)
    workflow.add_node("generate_answer", generate_answer_node)

    # è®¾ç½®å…¥å£ç‚¹
    workflow.add_edge(START, "analyze_intent")

    # æ·»åŠ æ¡ä»¶è¾¹
    workflow.add_conditional_edges(
        "analyze_intent",
        should_interrupt_after_intent,
        {
            "interrupt_check": "interrupt_check",
            "enhance_query": "enhance_query",
            "analyze_intent": "analyze_intent"  # å¯¹åº”Command(goto="analyze_intent")
        }
    )

    workflow.add_edge("interrupt_check", "analyze_intent")  # interruptåé‡æ–°åˆ†æ
    workflow.add_edge("enhance_query", "generate_answer")
    workflow.add_edge("generate_answer", END)

    # é…ç½®checkpointç”¨äºä¿å­˜çŠ¶æ€
    checkpointer = InMemorySaver()
    return workflow.compile(checkpointer=checkpointer)


def test_interrupt_workflow():
    """æµ‹è¯•å®Œæ•´çš„interruptå·¥ä½œæµ"""

    print("=" * 60)
    print("    LangGraph InterruptåŠŸèƒ½é›†æˆæµ‹è¯•")
    print("=" * 60)

    try:
        # åˆ›å»ºå·¥ä½œæµ
        graph = setup_test_graph()
        print("âœ… æµ‹è¯•å›¾æ„å»ºæˆåŠŸ")

        # æµ‹è¯•ç”¨ä¾‹1ï¼šæ˜ç¡®çš„æŸ¥è¯¢ï¼ˆåº”è¯¥æ­£å¸¸æ‰§è¡Œï¼‰
        test1_query = "ç»Ÿè®¡æµ™æ±Ÿçœæœ‰å¤šå°‘ä¸ª5Aæ™¯åŒº"
        print(f"\nğŸ“‹ æµ‹è¯•1 - æ˜ç¡®æŸ¥è¯¢: '{test1_query}'")

        config = {"configurable": {"thread_id": "test_thread_1"}}

        # æ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥æ˜ç¡®æŸ¥è¯¢çš„æƒ…å†µ
        def mock_interrupt_clear(context):
            return test1_query  # è¿”å›ç›¸åŒçš„æŸ¥è¯¢

        # ä¸´æ—¶æ›¿æ¢interruptå‡½æ•°
        import langgraph.types
        original_interrupt = langgraph.types.interrupt
        langgraph.types.interrupt = mock_interrupt_clear

        try:
            result1 = graph.invoke({"query": test1_query}, config=config)
            print(f"âœ… æµ‹è¯•ç»“æœ: {result1.get('status', 'unknown')}")
            print(f"ğŸ“ æœ€ç»ˆç­”æ¡ˆ: {result1.get('answer', 'æ— ')}")

            # æ£€æŸ¥æ€ç»´é“¾
            thought_chain = result1.get('thought_chain', [])
            for step in thought_chain:
                print(f"  - Step {step.get('step')}: {step.get('type')} - {step.get('action')}")
        finally:
            langgraph.types.interrupt = original_interrupt

        # æµ‹è¯•ç”¨ä¾‹2ï¼šä¸æ˜ç¡®çš„æŸ¥è¯¢ï¼ˆåº”è¯¥è§¦å‘interruptï¼‰
        test2_query = "æŸ¥è¯¢"  # è¿‡äºæ¨¡ç³Š
        print(f"\nğŸ“‹ æµ‹è¯•2 - ä¸æ˜ç¡®æŸ¥è¯¢: '{test2_query}'")

        config2 = {"configurable": {"thread_id": "test_thread_2"}}

        # æ¨¡æ‹Ÿç”¨æˆ·æ¾„æ¸…æŸ¥è¯¢çš„æƒ…å†µ
        clarified_query = "æŸ¥è¯¢æµ™æ±Ÿçœçš„5Aæ™¯åŒºæœ‰å“ªäº›"

        def mock_interrupt_unclear(context):
            print(f"ğŸ¤” LangGraph interruptè¢«è§¦å‘ï¼Œæ¨¡æ‹Ÿç”¨æˆ·æ¾„æ¸…æŸ¥è¯¢...")
            print(f"   åŸå§‹æŸ¥è¯¢: {context.get('original_query', '')}")
            print(f"   æç¤ºä¿¡æ¯: {context.get('message', '')}")
            return clarified_query  # è¿”å›æ¾„æ¸…åçš„æŸ¥è¯¢

        langgraph.types.interrupt = mock_interrupt_unclear

        try:
            result2 = graph.invoke({"query": test2_query}, config=config2)
            print(f"âœ… æµ‹è¯•ç»“æœ: {result2.get('status', 'unknown')}")
            print(f"ğŸ“ æœ€ç»ˆç­”æ¡ˆ: {result2.get('answer', 'æ— ')}")

            # éªŒè¯æœ€ç»ˆæŸ¥è¯¢æ˜¯å¦æ›´æ–°
            final_query = result2.get('query', '')
            if final_query == clarified_query:
                print(f"âœ… æŸ¥è¯¢æ¾„æ¸…æˆåŠŸ: '{test2_query}' -> '{final_query}'")
            else:
                print(f"âŒ æŸ¥è¯¢æœªæ­£ç¡®æ¾„æ¸…: {final_query}")

            # æ£€æŸ¥æ€ç»´é“¾
            thought_chain = result2.get('thought_chain', [])
            for step in thought_chain:
                print(f"  - Step {step.get('step')}: {step.get('type')} - {step.get('action')}")

        finally:
            langgraph.types.interrupt = original_interrupt

        print(f"\nğŸ¯ æµ‹è¯•æ€»ç»“:")
        print(f"âœ… Interruptæœºåˆ¶æ­£å¸¸è§¦å‘")
        print(f"âœ… æŸ¥è¯¢æ¾„æ¸…æµç¨‹å®Œæ•´æ‰§è¡Œ")
        print(f"âœ… é‡æ–°åˆ†æåŠŸèƒ½æ­£å¸¸å·¥ä½œ")

        return True

    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}", exc_info=True)
        return False


if __name__ == "__main__":
    success = test_interrupt_workflow()

    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ LangGraph InterruptåŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        print("âœ… å·¥ä½œæµèƒ½å¤Ÿåœ¨æŸ¥è¯¢ä¸æ˜ç¡®æ—¶è§¦å‘ä¸­æ–­")
        print("âœ… ç”¨æˆ·æ¾„æ¸…åå¯ä»¥é‡æ–°æ‰§è¡Œ")
    else:
        print("âš ï¸  æµ‹è¯•é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
    print("=" * 60)